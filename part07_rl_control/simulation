import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

# --- Actor-Critic ネットワーク定義 ---
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, max_action):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, action_dim), nn.Tanh()
        )
        self.max_action = max_action

    def forward(self, state):
        return self.max_action * self.net(state)

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 256), nn.ReLU(),
            nn.Linear(256, 256), nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        return self.net(x)

# --- リプレイバッファ ---
class ReplayBuffer:
    def __init__(self, max_size=100000):
        self.buffer = deque(maxlen=max_size)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, float(done)))

    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return map(lambda x: torch.FloatTensor(x), (state, action, reward, next_state, done))

# --- ハイパーパラメータ ---
ENV_NAME = "Pendulum-v1"
EPISODES = 200
MAX_STEPS = 200
GAMMA = 0.99
TAU = 0.005
BATCH_SIZE = 64
LR = 1e-3
NOISE_STD = 0.1

# --- 環境とネットワーク初期化 ---
env = gym.make(ENV_NAME)
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
max_action = float(env.action_space.high[0])

actor = Actor(state_dim, action_dim, max_action)
actor_target = Actor(state_dim, action_dim, max_action)
actor_target.load_state_dict(actor.state_dict())

critic = Critic(state_dim, action_dim)
critic_target = Critic(state_dim, action_dim)
critic_target.load_state_dict(critic.state_dict())

actor_optimizer = optim.Adam(actor.parameters(), lr=LR)
critic_optimizer = optim.Adam(critic.parameters(), lr=LR)
replay_buffer = ReplayBuffer()

# --- 学習ループ ---
for episode in range(EPISODES):
    state = env.reset()[0]
    episode_reward = 0

    for step in range(MAX_STEPS):
        state_tensor = torch.FloatTensor(state.reshape(1, -1))
        action = actor(state_tensor).detach().numpy()[0]
        action = (action + np.random.normal(0, NOISE_STD, size=action_dim)).clip(-max_action, max_action)

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        replay_buffer.add(state, action, reward, next_state, done)
        state = next_state
        episode_reward += reward

        if len(replay_buffer.buffer) >= BATCH_SIZE:
            s, a, r, s2, d = replay_buffer.sample(BATCH_SIZE)

            # Critic更新
            with torch.no_grad():
                target_q = critic_target(s2, actor_target(s2))
                y = r.unsqueeze(1) + GAMMA * (1 - d.unsqueeze(1)) * target_q
            q = critic(s, a)
            critic_loss = nn.MSELoss()(q, y)
            critic_optimizer.zero_grad()
            critic_loss.backward()
            critic_optimizer.step()

            # Actor更新
            actor_loss = -critic(s, actor(s)).mean()
            actor_optimizer.zero_grad()
            actor_loss.backward()
            actor_optimizer.step()

            # ターゲットネット更新
            for param, target_param in zip(critic.parameters(), critic_target.parameters()):
                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)
            for param, target_param in zip(actor.parameters(), actor_target.parameters()):
                target_param.data.copy_(TAU * param.data + (1 - TAU) * target_param.data)

        if done:
            break

    print(f"Episode {episode + 1}, Reward: {episode_reward:.2f}")

env.close()
