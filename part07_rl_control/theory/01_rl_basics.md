# 🧠 01. 強化学習の基本構造（RL Basics）

本節では、強化学習（Reinforcement Learning, RL）の基本構造と主要な用語・概念について解説します。  
RLは、エージェントが**環境と相互作用しながら最適な行動を学ぶ**という枠組みに基づきます。

---

## 🎯 強化学習の基本要素（MDP）

強化学習は、**マルコフ決定過程（Markov Decision Process, MDP）**として定式化されます。

| 要素 | 内容 |
|------|------|
| $S$  | 状態空間（State Space） |
| $A$  | 行動空間（Action Space） |
| $R$  | 報酬関数 $r(s, a)$ |
| $P$  | 遷移確率 $P(s'|s, a)$ |
| $\pi$ | ポリシー（方策）：状態→行動の写像 $\pi(a|s)$ |

---

## 🔄 学習の流れ（Agent–Environment Loop）

1. エージェントが状態 $s_t$ を観測  
2. ポリシー $\pi$ に基づいて行動 $a_t$ を選択  
3. 環境が新たな状態 $s_{t+1}$ と報酬 $r_t$ を返す  
4. エージェントが報酬に基づいて $\pi$ を更新  
5. 上記を繰り返しながら報酬最大化を目指す

---

## 📐 報酬の定義と目的関数

エージェントの目的は、**累積報酬**（return）を最大化することです。

- 割引累積報酬（将来報酬の重要度を減衰）  
  $G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}$
  - $\gamma \in [0, 1)$：割引率（例：0.99）

---

## 🧮 方策と価値関数

- 方策 $\pi(a|s)$：行動選択の戦略（確率分布または決定論的）
- 状態価値関数：  
  $V^\pi(s) = \mathbb{E}[G_t | s_t = s, \pi]$
- 行動価値関数：  
  $Q^\pi(s, a) = \mathbb{E}[G_t | s_t = s, a_t = a, \pi]$

---

## 🔧 モデルフリー vs モデルベース

| 種類         | 特徴 |
|--------------|------|
| モデルフリー | 環境モデルなし。Q学習やPolicy Gradient法など |
| モデルベース | 遷移モデルを内部的に推定・活用。MPCに近い考え方 |

---

## 🔍 主なアルゴリズム分類

| 系統         | 例 |
|--------------|----|
| 値ベース     | Q-learning, DQN |
| 方策ベース   | Policy Gradient, REINFORCE |
| Actor-Critic | A2C, DDPG, PPO（連続制御向け） |

---

## 🛠️ 制御応用におけるRLの特徴

- 数式モデルが不要（モデルフリー制御）  
- 適応性・柔軟性が高い（ノイズ・非線形性への耐性）  
- 訓練に時間がかかる、安定性保証が難しい  
- 古典制御とのハイブリッド設計が有効（PID × RL）

---

## 🔚 まとめ

強化学習は、「試行錯誤＋報酬」に基づいて制御戦略を獲得するアプローチです。  
次節では、倒立振子制御（CartPole）への適用を通して、実装と学習挙動を観察します。

📁 次へ：[`02_cartpole_ddpg.md`](./02_cartpole_ddpg.md)
